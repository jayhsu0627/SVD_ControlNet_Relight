11/28/2024 23:07:11 - INFO - __main__ - ***** Running training *****
11/28/2024 23:07:11 - INFO - __main__ -   Num examples = 4728
11/28/2024 23:07:11 - INFO - __main__ -   Num Epochs = 30
11/28/2024 23:07:11 - INFO - __main__ -   Instantaneous batch size per device = 2
11/28/2024 23:07:11 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64
11/28/2024 23:07:11 - INFO - __main__ -   Gradient Accumulation steps = 8
11/28/2024 23:07:11 - INFO - __main__ -   Total optimization steps = 2220
Steps:   0%|          | 0/2220 [00:00<?, ?it/s]/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]






Steps:   0%|          | 1/2220 [00:42<25:59:09, 42.16s/it, lr=0.0001, step_loss=0.441]11/28/2024 23:07:53 - INFO - __main__ - Running validation...
 Generating 1 videos.
/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'controlnet'} was not found in config. Values will be initialized to default values.
                                                                     Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-video-diffusion-img2vid.
{'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.
Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-video-diffusion-img2vid.
Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00, 61.11it/s]
(512, 512, 3) (512, 512, 1)
(512, 512, 3) (512, 512, 1)
(512, 512, 3) (512, 512, 1)
(512, 512, 3) (512, 512, 1)
(512, 512, 3) (512, 512, 1)
(512, 512, 3) (512, 512, 1)
Steps:   0%|          | 1/2220 [01:01<25:59:09, 42.16s/it, lr=0.0001, step_loss=0.102]/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]














































































































































Steps:   1%|          | 20/2220 [05:58<9:36:19, 15.72s/it, lr=0.0001, step_loss=0.0927]/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
11/28/2024 23:14:41 - INFO - __main__ - Step 20: Test Loss = 0.1926



























































































































































Steps:   2%|▏         | 40/2220 [12:44<9:28:52, 15.66s/it, lr=0.0001, step_loss=0.157]11/28/2024 23:21:26 - INFO - __main__ - Step 40: Test Loss = 0.1854




























































































































































Steps:   3%|▎         | 60/2220 [19:29<9:27:46, 15.77s/it, lr=0.0001, step_loss=0.277]11/28/2024 23:28:10 - INFO - __main__ - Step 60: Test Loss = 0.1848





















































































































































Steps:   4%|▎         | 80/2220 [26:31<9:57:20, 16.75s/it, lr=0.0001, step_loss=0.0957] 11/28/2024 23:35:13 - INFO - __main__ - Step 80: Test Loss = 0.1944

























































































































































Steps:   5%|▍         | 100/2220 [33:16<9:17:52, 15.79s/it, lr=0.0001, step_loss=0.294]11/28/2024 23:41:59 - INFO - __main__ - Step 100: Test Loss = 0.1582




















































































































































Steps:   5%|▌         | 120/2220 [40:01<9:11:24, 15.75s/it, lr=0.0001, step_loss=0.0607]11/28/2024 23:48:43 - INFO - __main__ - Step 120: Test Loss = 0.1770

























































































































































Steps:   6%|▋         | 140/2220 [46:46<9:08:49, 15.83s/it, lr=0.0001, step_loss=0.123]11/28/2024 23:55:27 - INFO - __main__ - Step 140: Test Loss = 0.1720






















































































































































Steps:   7%|▋         | 160/2220 [53:48<9:06:18, 15.91s/it, lr=0.0001, step_loss=0.118]11/29/2024 00:02:31 - INFO - __main__ - Step 160: Test Loss = 0.1746




















































































































































Steps:   8%|▊         | 180/2220 [1:00:32<8:52:40, 15.67s/it, lr=0.0001, step_loss=0.135]11/29/2024 00:09:13 - INFO - __main__ - Step 180: Test Loss = 0.1743























































































































































Steps:   9%|▉         | 200/2220 [1:07:15<8:48:14, 15.69s/it, lr=0.0001, step_loss=0.114]11/29/2024 00:15:57 - INFO - __main__ - Step 200: Test Loss = 0.1640
11/29/2024 00:15:57 - INFO - __main__ - Running validation...
 Generating 1 videos.
[2024-11-29 00:15:58,435] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /fs/nexus-scratch/sjxu/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
11/29/2024 00:16:00 - INFO - root - gcc -pthread -B /fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/include -I/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/include -fPIC -O2 -isystem /fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/include -fPIC -c /tmp/tmpn5fu5gza/test.c -o /tmp/tmpn5fu5gza/test.o
11/29/2024 00:16:00 - INFO - root - gcc -pthread -B /fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/compiler_compat /tmp/tmpn5fu5gza/test.o -laio -o /tmp/tmpn5fu5gza/a.out
Traceback (most recent call last):
  File "/fs/nexus-scratch/sjxu/svd-temporal-controlnet/train_svd_con.py", line 1883, in <module>
    vae=accelerator.unwrap_model(vae),
  File "/fs/nexus-scratch/sjxu/svd-temporal-controlnet/train_svd_con.py", line 1723, in main
    (global_step % args.validation_steps == 0)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/accelerate/accelerator.py", line 2540, in unwrap_model
    return extract_model_from_parallel(model, keep_fp32_wrapper)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/accelerate/utils/other.py", line 80, in extract_model_from_parallel
    from deepspeed import DeepSpeedEngine
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/deepspeed/__init__.py", line 25, in <module>
    from . import ops
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/deepspeed/ops/__init__.py", line 15, in <module>
    from ..git_version_info import compatible_ops as __compatible_ops__
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/deepspeed/git_version_info.py", line 29, in <module>
    op_compatible = builder.is_compatible()
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/deepspeed/ops/op_builder/fp_quantizer.py", line 35, in is_compatible
    sys_cuda_major, _ = installed_cuda_version()
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/deepspeed/ops/op_builder/builder.py", line 51, in installed_cuda_version
    raise MissingCUDAException("CUDA_HOME does not exist, unable to compile CUDA op(s)")
deepspeed.ops.op_builder.builder.MissingCUDAException: CUDA_HOME does not exist, unable to compile CUDA op(s)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/fs/nexus-scratch/sjxu/svd-temporal-controlnet/train_svd_con.py", line 1883, in <module>
[rank0]:     vae=accelerator.unwrap_model(vae),
[rank0]:   File "/fs/nexus-scratch/sjxu/svd-temporal-controlnet/train_svd_con.py", line 1723, in main
[rank0]:     (global_step % args.validation_steps == 0)
[rank0]:   File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/accelerate/accelerator.py", line 2540, in unwrap_model
[rank0]:     return extract_model_from_parallel(model, keep_fp32_wrapper)
[rank0]:   File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/accelerate/utils/other.py", line 80, in extract_model_from_parallel
[rank0]:     from deepspeed import DeepSpeedEngine
[rank0]:   File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/deepspeed/__init__.py", line 25, in <module>
[rank0]:     from . import ops
[rank0]:   File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/deepspeed/ops/__init__.py", line 15, in <module>
[rank0]:     from ..git_version_info import compatible_ops as __compatible_ops__
[rank0]:   File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/deepspeed/git_version_info.py", line 29, in <module>
[rank0]:     op_compatible = builder.is_compatible()
[rank0]:   File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/deepspeed/ops/op_builder/fp_quantizer.py", line 35, in is_compatible
[rank0]:     sys_cuda_major, _ = installed_cuda_version()
[rank0]:   File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/deepspeed/ops/op_builder/builder.py", line 51, in installed_cuda_version
[rank0]:     raise MissingCUDAException("CUDA_HOME does not exist, unable to compile CUDA op(s)")
[rank0]: deepspeed.ops.op_builder.builder.MissingCUDAException: CUDA_HOME does not exist, unable to compile CUDA op(s)