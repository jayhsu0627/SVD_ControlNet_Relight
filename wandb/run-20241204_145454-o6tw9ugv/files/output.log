INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = 319940
INFO:__main__:  Num Epochs = 30
INFO:__main__:  Instantaneous batch size per device = 2
INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:__main__:  Gradient Accumulation steps = 8
INFO:__main__:  Total optimization steps = 599910
Steps:   0%|                                                                                                                                                                                  | 0/599910 [00:00<?, ?it/s]
tensor(-1., device='cuda:0', dtype=torch.float16) tensor(1., device='cuda:0', dtype=torch.float16)
tensor(-1., device='cuda:0', dtype=torch.float16) tensor(1., device='cuda:0', dtype=torch.float16)
tensor(-1., device='cuda:0', dtype=torch.float16) tensor(1., device='cuda:0', dtype=torch.float16)

Steps:   0%|                                                                                                                                                           | 0/599910 [00:03<?, ?it/s, loss=0.107, lr=0.0001]
tensor(-1., device='cuda:0', dtype=torch.float16) tensor(1., device='cuda:0', dtype=torch.float16)
tensor(-1., device='cuda:0', dtype=torch.float16) tensor(1., device='cuda:0', dtype=torch.float16)
tensor(-1., device='cuda:0', dtype=torch.float16) tensor(1., device='cuda:0', dtype=torch.float16)

Steps:   0%|                                                                                                                                                           | 0/599910 [00:05<?, ?it/s, loss=0.092, lr=0.0001]
tensor(-1., device='cuda:0', dtype=torch.float16) tensor(1., device='cuda:0', dtype=torch.float16)
tensor(-1., device='cuda:0', dtype=torch.float16) tensor(1., device='cuda:0', dtype=torch.float16)
tensor(-0.8823, device='cuda:0', dtype=torch.float16) tensor(1., device='cuda:0', dtype=torch.float16)

Steps:   0%|                                                                                                                                                            | 0/599910 [00:08<?, ?it/s, loss=0.08, lr=0.0001]
tensor(-1., device='cuda:0', dtype=torch.float16) tensor(1., device='cuda:0', dtype=torch.float16)
tensor(-1., device='cuda:0', dtype=torch.float16) tensor(1., device='cuda:0', dtype=torch.float16)
tensor(-1., device='cuda:0', dtype=torch.float16) tensor(1., device='cuda:0', dtype=torch.float16)
Steps:   0%|                                                                                                                                                           | 0/599910 [00:09<?, ?it/s, loss=0.151, lr=0.0001]Traceback (most recent call last):
  File "/fs/nexus-scratch/sjxu/svd-temporal-controlnet/train_svd_decoder.py", line 1675, in <module>
    main()
  File "/fs/nexus-scratch/sjxu/svd-temporal-controlnet/train_svd_decoder.py", line 1622, in main
    optimizer.step()
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/accelerate/optimizer.py", line 157, in step
    self.scaler.step(self.optimizer, closure)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/amp/grad_scaler.py", line 448, in step
    self.unscale_(optimizer)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/amp/grad_scaler.py", line 338, in unscale_
    optimizer_state["found_inf_per_device"] = self._unscale_grads_(
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/amp/grad_scaler.py", line 260, in _unscale_grads_
    raise ValueError("Attempting to unscale FP16 gradients.")
ValueError: Attempting to unscale FP16 gradients.