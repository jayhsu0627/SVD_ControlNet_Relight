INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = 4728
INFO:__main__:  Num Epochs = 120
INFO:__main__:  Instantaneous batch size per device = 1
INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 64
INFO:__main__:  Gradient Accumulation steps = 16
INFO:__main__:  Total optimization steps = 8880
Steps:   0%|          | 0/8880 [00:00<?, ?it/s]
before controlnet: torch.Size([1, 6, 8, 64, 64]) torch.Size([1]) torch.Size([1, 77, 1024]) torch.Size([1, 3]) torch.Size([1, 6, 4, 512, 512]) torch.Size([1, 6, 1, 16])
1 6
emb torch.Size([1, 320]) torch.Size([1, 96])
emb torch.Size([1, 1280])

Steps:   0%|          | 0/8880 [00:11<?, ?it/s, lr=0.0001, step_loss=0.0565]
before controlnet: torch.Size([1, 6, 8, 64, 64]) torch.Size([1]) torch.Size([1, 77, 1024]) torch.Size([1, 3]) torch.Size([1, 6, 4, 512, 512]) torch.Size([1, 6, 1, 16])
1 6
emb torch.Size([1, 320]) torch.Size([1, 96])
emb torch.Size([1, 1280])

Steps:   0%|          | 0/8880 [00:14<?, ?it/s, lr=0.0001, step_loss=0.16]
before controlnet: torch.Size([1, 6, 8, 64, 64]) torch.Size([1]) torch.Size([1, 77, 1024]) torch.Size([1, 3]) torch.Size([1, 6, 4, 512, 512]) torch.Size([1, 6, 1, 16])
1 6
emb torch.Size([1, 320]) torch.Size([1, 96])
emb torch.Size([1, 1280])
before downsample: torch.Size([6, 320, 64, 64]) torch.Size([6, 1280]) torch.Size([6, 77, 1024])
before controlnet: torch.Size([1, 6, 8, 64, 64]) torch.Size([1]) torch.Size([1, 77, 1024]) torch.Size([1, 3]) torch.Size([1, 6, 4, 512, 512]) torch.Size([1, 6, 1, 16])
1 6
emb torch.Size([1, 320]) torch.Size([1, 96])
emb torch.Size([1, 1280])

Steps:   0%|          | 0/8880 [00:15<?, ?it/s, lr=0.0001, step_loss=0.526]
before controlnet: torch.Size([1, 6, 8, 64, 64]) torch.Size([1]) torch.Size([1, 77, 1024]) torch.Size([1, 3]) torch.Size([1, 6, 4, 512, 512]) torch.Size([1, 6, 1, 16])
1 6
emb torch.Size([1, 320]) torch.Size([1, 96])
emb torch.Size([1, 1280])

Steps:   0%|          | 0/8880 [00:18<?, ?it/s, lr=0.0001, step_loss=0.523]
before controlnet: torch.Size([1, 6, 8, 64, 64]) torch.Size([1]) torch.Size([1, 77, 1024]) torch.Size([1, 3]) torch.Size([1, 6, 4, 512, 512]) torch.Size([1, 6, 1, 16])
1 6
emb torch.Size([1, 320]) torch.Size([1, 96])
emb torch.Size([1, 1280])
before downsample: torch.Size([6, 320, 64, 64]) torch.Size([6, 1280]) torch.Size([6, 77, 1024])
before controlnet: torch.Size([1, 6, 8, 64, 64]) torch.Size([1]) torch.Size([1, 77, 1024]) torch.Size([1, 3]) torch.Size([1, 6, 4, 512, 512]) torch.Size([1, 6, 1, 16])
1 6
emb torch.Size([1, 320]) torch.Size([1, 96])
emb torch.Size([1, 1280])

Steps:   0%|          | 0/8880 [00:19<?, ?it/s, lr=0.0001, step_loss=0.445]
before controlnet: torch.Size([1, 6, 8, 64, 64]) torch.Size([1]) torch.Size([1, 77, 1024]) torch.Size([1, 3]) torch.Size([1, 6, 4, 512, 512]) torch.Size([1, 6, 1, 16])
1 6
emb torch.Size([1, 320]) torch.Size([1, 96])
emb torch.Size([1, 1280])

Steps:   0%|          | 0/8880 [00:22<?, ?it/s, lr=0.0001, step_loss=0.147]
before controlnet: torch.Size([1, 6, 8, 64, 64]) torch.Size([1]) torch.Size([1, 77, 1024]) torch.Size([1, 3]) torch.Size([1, 6, 4, 512, 512]) torch.Size([1, 6, 1, 16])
1 6
emb torch.Size([1, 320]) torch.Size([1, 96])
emb torch.Size([1, 1280])
before downsample: torch.Size([6, 320, 64, 64]) torch.Size([6, 1280]) torch.Size([6, 77, 1024])
before controlnet: torch.Size([1, 6, 8, 64, 64]) torch.Size([1]) torch.Size([1, 77, 1024]) torch.Size([1, 3]) torch.Size([1, 6, 4, 512, 512]) torch.Size([1, 6, 1, 16])
1 6
emb torch.Size([1, 320]) torch.Size([1, 96])
emb torch.Size([1, 1280])

Steps:   0%|          | 0/8880 [00:23<?, ?it/s, lr=0.0001, step_loss=0.137]
before controlnet: torch.Size([1, 6, 8, 64, 64]) torch.Size([1]) torch.Size([1, 77, 1024]) torch.Size([1, 3]) torch.Size([1, 6, 4, 512, 512]) torch.Size([1, 6, 1, 16])
1 6
emb torch.Size([1, 320]) torch.Size([1, 96])
emb torch.Size([1, 1280])

Steps:   0%|          | 0/8880 [00:26<?, ?it/s, lr=0.0001, step_loss=0.0818]
before controlnet: torch.Size([1, 6, 8, 64, 64]) torch.Size([1]) torch.Size([1, 77, 1024]) torch.Size([1, 3]) torch.Size([1, 6, 4, 512, 512]) torch.Size([1, 6, 1, 16])
1 6
emb torch.Size([1, 320]) torch.Size([1, 96])
emb torch.Size([1, 1280])
before downsample: torch.Size([6, 320, 64, 64]) torch.Size([6, 1280]) torch.Size([6, 77, 1024])
before controlnet: torch.Size([1, 6, 8, 64, 64]) torch.Size([1]) torch.Size([1, 77, 1024]) torch.Size([1, 3]) torch.Size([1, 6, 4, 512, 512]) torch.Size([1, 6, 1, 16])
1 6
emb torch.Size([1, 320]) torch.Size([1, 96])
emb torch.Size([1, 1280])

Steps:   0%|          | 0/8880 [00:27<?, ?it/s, lr=0.0001, step_loss=0.171]
before controlnet: torch.Size([1, 6, 8, 64, 64]) torch.Size([1]) torch.Size([1, 77, 1024]) torch.Size([1, 3]) torch.Size([1, 6, 4, 512, 512]) torch.Size([1, 6, 1, 16])
1 6
emb torch.Size([1, 320]) torch.Size([1, 96])
emb torch.Size([1, 1280])

Steps:   0%|          | 0/8880 [00:30<?, ?it/s, lr=0.0001, step_loss=0.215]
before controlnet: torch.Size([1, 6, 8, 64, 64]) torch.Size([1]) torch.Size([1, 77, 1024]) torch.Size([1, 3]) torch.Size([1, 6, 4, 512, 512]) torch.Size([1, 6, 1, 16])
1 6
emb torch.Size([1, 320]) torch.Size([1, 96])
emb torch.Size([1, 1280])
before downsample: torch.Size([6, 320, 64, 64]) torch.Size([6, 1280]) torch.Size([6, 77, 1024])
before controlnet: torch.Size([1, 6, 8, 64, 64]) torch.Size([1]) torch.Size([1, 77, 1024]) torch.Size([1, 3]) torch.Size([1, 6, 4, 512, 512]) torch.Size([1, 6, 1, 16])
1 6
emb torch.Size([1, 320]) torch.Size([1, 96])
emb torch.Size([1, 1280])
Steps:   0%|          | 1/8880 [00:31<78:51:09, 31.97s/it, lr=0.0001, step_loss=0.215]INFO:__main__:Running validation...
 Generating 1 videos.
{'controlnet', 'insert_light'} was not found in config. Values will be initialized to default values.
                                                                     {'rescale_betas_zero_snr', 'final_sigmas_type'} was not found in config. Values will be initialized to default values.
Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-video-diffusion-img2vid.
Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-video-diffusion-img2vid.
Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00, 279.26it/s]
(512, 512, 3) (512, 512, 1)
(512, 512, 3) (512, 512, 1)
(512, 512, 3) (512, 512, 1)
(512, 512, 3) (512, 512, 1)
(512, 512, 3) (512, 512, 1)
(512, 512, 3) (512, 512, 1)
Traceback (most recent call last):
  File "/fs/nexus-scratch/sjxu/svd-temporal-controlnet/train_svd_controlnet.py", line 1957, in <module>
    main()
  File "/fs/nexus-scratch/sjxu/svd-temporal-controlnet/train_svd_controlnet.py", line 1839, in main
    video_frames = pipeline(
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/fs/nexus-scratch/sjxu/svd-temporal-controlnet/pipeline/pipeline_stable_video_diffusion_controlnet.py", line 543, in __call__
    down_block_res_samples, mid_block_res_sample = self.controlnet(
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/amp/autocast_mode.py", line 43, in decorate_autocast
    return func(*args, **kwargs)
  File "/fs/nexus-scratch/sjxu/svd-temporal-controlnet/models/controlnet_sdv.py", line 513, in forward
    emb = self.time_embedding(t_emb, timestep_cond)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/diffusers/models/embeddings.py", line 375, in forward
    sample = sample + self.cond_proj(condition)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 117, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (6x16 and 96x320)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/fs/nexus-scratch/sjxu/svd-temporal-controlnet/train_svd_controlnet.py", line 1957, in <module>
[rank0]:     main()
[rank0]:   File "/fs/nexus-scratch/sjxu/svd-temporal-controlnet/train_svd_controlnet.py", line 1839, in main
[rank0]:     video_frames = pipeline(
[rank0]:   File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/fs/nexus-scratch/sjxu/svd-temporal-controlnet/pipeline/pipeline_stable_video_diffusion_controlnet.py", line 543, in __call__
[rank0]:     down_block_res_samples, mid_block_res_sample = self.controlnet(
[rank0]:   File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/accelerate/utils/operations.py", line 819, in forward
[rank0]:     return model_forward(*args, **kwargs)
[rank0]:   File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/accelerate/utils/operations.py", line 807, in __call__
[rank0]:     return convert_to_fp32(self.model_forward(*args, **kwargs))
[rank0]:   File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/amp/autocast_mode.py", line 43, in decorate_autocast
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/fs/nexus-scratch/sjxu/svd-temporal-controlnet/models/controlnet_sdv.py", line 513, in forward
[rank0]:     emb = self.time_embedding(t_emb, timestep_cond)
[rank0]:   File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/diffusers/models/embeddings.py", line 375, in forward
[rank0]:     sample = sample + self.cond_proj(condition)
[rank0]:   File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/fs/nexus-scratch/sjxu/miniconda3/envs/svd_control/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 117, in forward
[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]: RuntimeError: mat1 and mat2 shapes cannot be multiplied (6x16 and 96x320)
2 6
emb torch.Size([2, 320]) torch.Size([6, 16])